{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b792943-438a-4e16-ab92-7ffb909b8091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Brief PySpark on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94be76b4-6b12-46c4-a83e-32916cc34882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Merge les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bd200f-5163-47dc-9ede-f0e6decb8891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Charger les bibliothèques Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ef130f-bac7-4940-8ca2-baf6523c0add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"NettoyageDonnees\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b476bab-52f6-4c96-a0b8-19146114bb21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Charger les tables\n",
    "df1 = spark.table(\"default.yellow_tripdata_2024_01\")\n",
    "df2 = spark.table(\"default.yellow_tripdata_2024_02\")\n",
    "df3 = spark.table(\"default.yellow_tripdata_2024_03\")\n",
    "df4 = spark.table(\"default.yellow_tripdata_2024_04\")\n",
    "df5 = spark.table(\"default.yellow_tripdata_2024_05\")\n",
    "df6 = spark.table(\"default.yellow_tripdata_2024_06\")\n",
    "df7 = spark.table(\"default.yellow_tripdata_2024_07\")\n",
    "df8 = spark.table(\"default.yellow_tripdata_2024_08\")\n",
    "df9 = spark.table(\"default.yellow_tripdata_2024_09\")\n",
    "df10 = spark.table(\"default.yellow_tripdata_2024_10\")\n",
    "df11 = spark.table(\"default.yellow_tripdata_2024_11\")\n",
    "df12 = spark.table(\"default.yellow_tripdata_2024_12\")\n",
    "df13 = spark.table(\"default.yellow_tripdata_2025_01\")\n",
    "df14 = spark.table(\"default.yellow_tripdata_2025_02\")\n",
    "df15 = spark.table(\"default.yellow_tripdata_2025_03\")\n",
    "df16 = spark.table(\"default.yellow_tripdata_2025_04\")\n",
    "df17 = spark.table(\"default.yellow_tripdata_2025_05\")\n",
    "df18 = spark.table(\"default.yellow_tripdata_2025_06\")\n",
    "df19 = spark.table(\"default.yellow_tripdata_2025_07\")\n",
    "df20 = spark.table(\"default.yellow_tripdata_2025_08\")\n",
    "df21 = spark.table(\"default.yellow_tripdata_2025_09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a335229-e812-44f9-9626-1f090bb705e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Trouver toutes les colonnes présentes dans au moins une table\n",
    "all_columns = set(df1.columns) | set(df2.columns) | set(df3.columns) | set(df4.columns) | set(df5.columns) | set(df6.columns) | set(df7.columns) | set(df8.columns) | set(df9.columns) | set(df10.columns) | set(df11.columns) | set(df12.columns) | set(df13.columns) | set(df14.columns) | set(df15.columns) | set(df16.columns) | set(df17.columns) | set(df18.columns) | set(df19.columns) | set(df20.columns) | set(df21.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a803855-3f9f-4576-a3c7-04d22853b772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fonction pour ajouter les colonnes manquantes\n",
    "def add_missing_cols(df, all_cols):\n",
    "    existing_cols = set(df.columns)\n",
    "    for c in all_cols - existing_cols:\n",
    "        df = df.withColumn(c, lit(None))\n",
    "    return df.select(sorted(all_cols)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d69933-c86e-4547-aaa3-cdd0bc5d296f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ajouter les colonnes manquantes\n",
    "df1 = add_missing_cols(df1, all_columns)\n",
    "df2 = add_missing_cols(df2, all_columns)\n",
    "df3 = add_missing_cols(df3, all_columns)\n",
    "df4 = add_missing_cols(df4, all_columns)\n",
    "df5 = add_missing_cols(df5, all_columns)\n",
    "df6 = add_missing_cols(df6, all_columns)\n",
    "df7 = add_missing_cols(df7, all_columns)\n",
    "df8 = add_missing_cols(df8, all_columns)\n",
    "df9 = add_missing_cols(df9, all_columns)\n",
    "df10 = add_missing_cols(df10, all_columns)\n",
    "df11 = add_missing_cols(df11, all_columns)\n",
    "df12 = add_missing_cols(df12, all_columns)\n",
    "df13 = add_missing_cols(df13, all_columns)\n",
    "df14 = add_missing_cols(df14, all_columns)\n",
    "df15 = add_missing_cols(df15, all_columns)\n",
    "df16 = add_missing_cols(df16, all_columns)\n",
    "df17 = add_missing_cols(df17, all_columns)\n",
    "df18 = add_missing_cols(df18, all_columns)\n",
    "df19 = add_missing_cols(df19, all_columns)\n",
    "df20 = add_missing_cols(df20, all_columns)\n",
    "df21 = add_missing_cols(df21, all_columns)\n",
    "\n",
    "merged_df1 = df1.union(df2)\n",
    "merged_df2 = merged_df1.union(df3)\n",
    "merged_df3 = merged_df2.union(df4)\n",
    "merged_df4 = merged_df3.union(df5)\n",
    "merged_df5 = merged_df4.union(df6)\n",
    "merged_df6 = merged_df5.union(df7)\n",
    "merged_df7 = merged_df6.union(df8)\n",
    "merged_df8 = merged_df7.union(df9)\n",
    "merged_df9 = merged_df8.union(df10)\n",
    "merged_df10 = merged_df9.union(df11)\n",
    "merged_df11 = merged_df10.union(df12)\n",
    "merged_df12 = merged_df11.union(df13)\n",
    "merged_df13 = merged_df12.union(df14)\n",
    "merged_df14 = merged_df13.union(df15)\n",
    "merged_df15 = merged_df14.union(df16)\n",
    "merged_df16 = merged_df15.union(df17)\n",
    "merged_df17 = merged_df16.union(df18)\n",
    "merged_df18 = merged_df17.union(df19)\n",
    "merged_df19 = merged_df18.union(df20)\n",
    "\n",
    "df = merged_df19.union(df21)\n",
    "\n",
    "df.write \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"default.Raw_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f90ccb-88ac-45e3-9144-90d085afee7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0cd1dfa-4468-45a2-95fa-4ca41f61cd3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Nettoyer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b276525-10ca-43ae-8e56-3ef304de7485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE default\")\n",
    "\n",
    "# Lister les tables disponibles dans la base\n",
    "spark.sql(\"SHOW TABLES\").display()\n",
    "\n",
    "# Récupération de la table raw\n",
    "df1 = spark.table(\"default.Raw_table\")\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Trouver les lignes dupliquées\n",
    "duplicates = (\n",
    "    df1.groupBy(df1.columns)\n",
    "      .count()\n",
    "      .filter(F.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "duplicates.display(truncate=False)\n",
    "print(\"Nombre de doublons :\", duplicates.count())\n",
    "\n",
    "df_clean = df1.dropDuplicates()\n",
    "df_clean.write.mode(\"overwrite\").saveAsTable(\"default.clean_table\")\n",
    "\n",
    "df_clean.display(5)\n",
    "print('Nombre de lignes : ', df_clean.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87cad63-6ec8-4a1e-99be-0311c69d165e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Créer les tables des requêtes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced1de65-f9e3-407c-9a33-4420b8408a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"USE default\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").display()\n",
    "\n",
    "df1 = spark.table(\"default.clean_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0a109c9-d08c-4f6c-8c9d-328781811127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Identifier les 10 zones de départ les plus fréquentées chaque mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1927d2-ac55-4660-b9b9-262c1e05357c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS default.Top_10_pickup_zones AS\n",
    "    WITH trips_per_month AS (\n",
    "        SELECT \n",
    "            YEAR(tpep_pickup_datetime) AS year,\n",
    "            MONTH(tpep_pickup_datetime) AS month,\n",
    "            PULocationID,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM default.clean_table\n",
    "        GROUP BY year, month, PULocationID\n",
    "    ),\n",
    "    ranking AS (\n",
    "        SELECT \n",
    "            year,\n",
    "            month,\n",
    "            PULocationID,\n",
    "            trip_count,\n",
    "            RANK() OVER (PARTITION BY year, month ORDER BY trip_count DESC) AS rank\n",
    "        FROM trips_per_month\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM ranking\n",
    "    WHERE rank <= 10\n",
    "    ORDER BY year, month, rank\n",
    "\"\"\")\n",
    "display(spark.table(\"default.Top_10_pickup_zones\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922e0460-4cbc-4fcb-860a-79045093c279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Calculer la durée moyenne des trajets par mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e08682-6f07-43f2-83bb-61aee5e75540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS default.avg_trip_duration_per_month AS\n",
    "    SELECT \n",
    "        CONCAT(YEAR(tpep_pickup_datetime), '-', MONTH(tpep_pickup_datetime)) AS datemonth,\n",
    "        ROUND(AVG(UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime)), 2) AS duration\n",
    "    FROM default.clean_table\n",
    "    GROUP BY datemonth\n",
    "    ORDER BY datemonth\n",
    "\"\"\")\n",
    "\n",
    "df = spark.table(\"default.avg_trip_duration_per_month\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b899ec6-147f-470f-9a41-d306b503b972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Déterminer la distance moyenne par type de paiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf07a01-5fed-460f-bfe9-7fe2d8819323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS default.avg_distance_by_payment_type AS\n",
    "        SELECT payment_type, avg(trip_distance) as avg_distance\n",
    "        FROM default.clean_table\n",
    "        GROUP BY payment_type\n",
    "        ORDER BY avg_distance\"\"\")\n",
    "display(spark.table(\"default.avg_distance_by_payment_type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee680fbf-688d-40bd-8d93-1a95deedaceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Estimer le montant moyen des courses en fonction du nombre de passagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a940ac60-8a67-42e6-8b06-1fdcb8fa8a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        DROP TABLE IF EXISTS default.avg_amount_by_passenger_count\"\"\")\n",
    "spark.sql(\"\"\" \n",
    "        CREATE TABLE IF NOT EXISTS default.avg_amount_by_passenger_count AS\n",
    "        SELECT round(avg(total_amount), 2) as avg_total_amount, passenger_count\n",
    "        FROM default.clean_table\n",
    "        GROUP BY passenger_count\n",
    "        ORDER BY passenger_count;\"\"\")\n",
    "display(spark.table(\"default.avg_amount_by_passenger_count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb4f540-ab5c-491c-8656-8b7bcc7070e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mesurer la somme totale des pourboires versés chaque mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d776d9-82fa-447a-952c-9fb5682cc001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "        DROP TABLE IF EXISTS default.total_tip_per_month\"\"\")\n",
    "spark.sql(\"\"\" \n",
    "        CREATE TABLE IF NOT EXISTS default.total_tip_per_month AS\n",
    "        SELECT round(SUM(tip_amount), 2) as total_tip_amount, concat(year(tpep_pickup_datetime), '-', month(tpep_pickup_datetime)) as month_year\n",
    "        FROM default.clean_table\n",
    "        GROUP BY concat(year(tpep_pickup_datetime), '-', month(tpep_pickup_datetime))\n",
    "        ORDER BY concat(year(tpep_pickup_datetime), '-', month(tpep_pickup_datetime));\"\"\")\n",
    "display(spark.table(\"default.total_tip_per_month\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5a0f49-d3e4-4c21-9113-ad7018f56bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4.Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b84e2b-e616-48a9-b92b-041a16494a05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "jdbc_username = os.environ.get(\"LOGIN\")\n",
    "jdbc_password = os.environ.get(\"PASS\")\n",
    "\n",
    "print(\"Utilisateur connecté :\", jdbc_username)\n",
    "\n",
    "jdbc_url = \"jdbc:sqlserver://cacfsql.database.windows.net:1433;database=brief\"\n",
    "\n",
    "connection_properties = {\n",
    "    \"user\": jdbc_username,\n",
    "    \"password\": jdbc_password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88101294-333a-4472-bc5e-0a1eca984f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = [\"total_tip_per_month\", \"avg_distance_by_payment_type\", \"avg_trip_duration_per_month\", \"avg_amount_by_passenger_count\", \"top_10_pickup_zones\"]\n",
    "\n",
    "for table in tables:\n",
    "    df = spark.table(\"default.\" + table)\n",
    "    df.write.mode(\"overwrite\").jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"lr_\" + table,\n",
    "        properties={\n",
    "            \"user\": jdbc_username,\n",
    "            \"password\": jdbc_password,\n",
    "            \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3084535d-aa53-45fb-a7bb-0621e583e2a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sparkexercice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
